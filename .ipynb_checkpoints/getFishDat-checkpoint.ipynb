{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: {sys.executable}: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "#import sys\n",
    "!{sys.executable} -m pip install flickrapi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flickrapi import FlickrAPI\n",
    "import pandas as pd\n",
    "#import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4d860f720718fdce373fe644da0d61b4\n",
      "32\n",
      "4d860f720718fdce373fe644da0d61b4\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "#set \"key\" and \"secret\" in a local configuration file named flickr_APIkey.txt\n",
    "#for an example see: flickr_APIkey_example.txt\n",
    "#read-in key and secret from configuration file\n",
    "file=open(file=\"flickr_APIkey.txt\", mode=\"r\") #mode r = read the file\n",
    "key=file.readline()\n",
    "key=key.rstrip() # Remove all trailing whitespace (e.g., \\n)\n",
    "secret=file.readline()\n",
    "secret=secret.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(image_tag, MAX_COUNT):    \n",
    "    flickr = FlickrAPI(key1, secret)\n",
    "    photos = flickr.walk(text=image_tag,\n",
    "                            tag_mode='all',\n",
    "                            #tags=image_tag, #use text parameter instead, which searches title, description, and tags\n",
    "                            has_geo=1,\n",
    "                            lat=41.604168,\n",
    "                            lon=-71.320683,\n",
    "                            extras='date_upload, date_taken, geo, url_o',\n",
    "                            per_page=50,\n",
    "                            sort='relevance'\n",
    "                        ) \n",
    "    count=0\n",
    "    urls=[]\n",
    "    uploads=[]\n",
    "    geos=[]\n",
    "    takens=[]\n",
    "    for photo in photos:\n",
    "        if count< MAX_COUNT:\n",
    "            count=count+1\n",
    "            print(\"Fetching url for image number {}\".format(count))\n",
    "            try:\n",
    "                url=photo.get('url_o')\n",
    "                urls.append(url)\n",
    "                upload=photo.get('date_upload')\n",
    "                uploads.append(upload)\n",
    "                taken=photo.get('date_taken')\n",
    "                takens.append(taken)\n",
    "                geo=photo.get('geo')\n",
    "                geos.append(geo)\n",
    "            except:\n",
    "                print(\"Url for image number {} could not be fetched\".format(count))\n",
    "        else:\n",
    "            print(\"Done fetching urls, fetched {} urls out of {}\".format(len(urls),MAX_COUNT))\n",
    "            break\n",
    "    urls=pd.Series(urls)\n",
    "    uploads=pd.Series(uploads)\n",
    "    geos=pd.Series(geos)\n",
    "    takens=pd.Series(takens)\n",
    "    print(\"Writing out the urls in the current directory\")\n",
    "    urls.to_csv(image_tag+\"_urls.csv\")\n",
    "    uploads.to_csv(image_tag+\"_uploads.csv\")\n",
    "    takens.to_csv(image_tag+\"_takens.csv\")\n",
    "    geos.to_csv(image_tag+\"_geos.csv\")\n",
    "    print(\"Done!!!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching url for image number 1\n",
      "Fetching url for image number 2\n",
      "Fetching url for image number 3\n",
      "Fetching url for image number 4\n",
      "Fetching url for image number 5\n",
      "Fetching url for image number 6\n",
      "Fetching url for image number 7\n",
      "Fetching url for image number 8\n",
      "Fetching url for image number 9\n",
      "Fetching url for image number 10\n",
      "Fetching url for image number 11\n",
      "Fetching url for image number 12\n",
      "Fetching url for image number 13\n",
      "Fetching url for image number 14\n",
      "Fetching url for image number 15\n",
      "Fetching url for image number 16\n",
      "Fetching url for image number 17\n",
      "Fetching url for image number 18\n",
      "Fetching url for image number 19\n",
      "Writing out the urls in the current directory\n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "#image_tag=\"fish\"\n",
    "#MAX_COUNT=100\n",
    "get_urls(\"fish\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only need the following code if running this from Terminal\n",
    "#def main(): \n",
    "#    tag=sys.argv[1]\n",
    "#    MAX_COUNT=int(sys.argv[2])\n",
    "#    get_urls(tag,MAX_COUNT)\n",
    "\n",
    "\n",
    "#if __name__=='__main__': \n",
    "#    main() \n",
    "# if running the module as the main program (e.g., in terminal: python getFishDat.py): execute function (main)\n",
    "# if module is imported by another program, _name_ is assigned to the module name \"getFishDat\": skip function (main)\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "def put_images(image_tag):\n",
    "    urls=[]\n",
    "    FILE_NAME=image_tag+\"_urls.csv\"\n",
    "    with open(FILE_NAME,newline=\"\") as csvfile:\n",
    "        doc=csv.reader(csvfile,delimiter=\",\")\n",
    "        for row in doc:\n",
    "            if row[1].startswith(\"https\"):\n",
    "                urls.append(row[1])\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(),FILE_NAME.split(\"_\")[0])):\n",
    "        os.mkdir(FILE_NAME.split(\"_\")[0])\n",
    "    t0=time.time()\n",
    "    for url in enumerate(urls):\n",
    "        print(\"Starting download {} of \".format(url[0]+1),len(urls))\n",
    "        try:\n",
    "            resp=requests.get(url[1],stream=True)\n",
    "            path_to_write=os.path.join(os.getcwd(),FILE_NAME.split(\"_\")[0],url[1].split(\"/\")[-1])\n",
    "            outfile=open(path_to_write,'wb')\n",
    "            outfile.write(resp.content)\n",
    "            outfile.close()\n",
    "            print(\"Done downloading {} of {}\".format(url[0]+1,len(urls)))\n",
    "        except:\n",
    "            print(\"Failed to download url number {}\".format(url[0]))\n",
    "    t1=time.time()\n",
    "    print(\"Done with download, job took {} seconds\".format(t1-t0))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "put_images(image_tag=image_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
